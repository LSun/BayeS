---
title: "SBRr Compared with Bayesian Bridge and Spike-and-Slab MCMC"
author: "Lei Sun"
date: 2017-02-10
output: html_document
---

```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

```{r knitr-opts-chunk, include=FALSE}
```

**Last updated:** `r Sys.Date()`

**Code version:** `r workflowr::extract_commit(".", 1)$sha1`

## Introduction

Using simulated data with correlated design matrix $X$, we compare `SBRr` with [`BayesBridge`] by [Polson et al., 2014] and [`BoomSpikeSlab`] by Steven L. Scott, on estimating multiple linear regression coefficients and selecting relevant variables in a high-dimensional setting.

```{r, message = FALSE, include = FALSE}
library(glmnet)
library(BayesBridge)
library(BoomSpikeSlab)
source("../code/SBRr.R")
library(mvtnorm)
```

```{r, include = FALSE}
sparrep = function(x) {
  position = which(abs(x) > 0)
  value = x[position]
  return(cbind(position, value))
}

varseleval = function(x, y = tp_beta) {
  tp = intersect(x, y)
  fp = setdiff(x, y)
  tpn = length(tp)
  fpn = length(fp)
  return(list(tp = tp, fp = fp, tpn = tpn, fpn = fpn))
}

vec.norm = function (x) {
  if(all(x == 0)) {stop} else {x.norm = (x - mean(x)) / sqrt(sum((x - mean(x))^2)); return(x.norm)}
}

cv.sbr = function (A, z, lambda, nfold = 10) {
  n = nrow(A)
  p = ncol(A)
  fold = split(sample(n, n), 1:nfold)
  mse_cv = mse_mean = mse_sd = c()
  for (j in 1:length(lambda)) {
    for (i in 1:nfold) {
      betahat.sbr = sbr(A = A[-fold[[i]], ], z = z[-fold[[i]]], lambda[j])
      betahat = betahat.sbr$x
      # mse_cv[i] = mean((A[fold[[i]], ]%*%betahat - z[fold[[i]]])^2)
      betahat_active = betahat.sbr$x.sparse[, 1]
      betahat_active_est = coef(lm(z[-fold[[i]]] ~ A[-fold[[i]], betahat_active] - 1))
      mse_cv[i] = mean((A[fold[[i]], betahat_active] %*% betahat_active_est - z[fold[[i]]])^2)
    }
    mse_mean[j] = mean(mse_cv)
    mse_sd[j] = sd(mse_cv)
  }
  return(list(lambda = lambda, mse_mean = mse_mean, mse_sd = mse_sd))
}

sparserep = function (beta) {
  position = which(abs(beta) > 0)
  value = beta[position]
  return(cbind(position, value))
}
```

## $L_0$ regularization is relatively robust to the choice of $\lambda$.

Each row of the design matrix $X_{n\times p}$ is iid $N(0, \Sigma_p)$, where $\Sigma_p = B_{p\times d}B_{d\times p}^T + I_p$, $B_{ij} \sim N(0, 1)$, $d\ll p$, in order to impose high collinearity among the columns of $X$.  In addition, $X$ is column-wise normalized such tht for each column $X_j$, $\bar X_j = 0$, $\|X_j\|_2 = 1$.  The coefficients $\beta$ is generated such that $\eta = 90\%$ of them are zero, and $1 - \eta = 10\%$ of them are equally spaced from zero in both negative and positive directions.  Under this setting, the signal-to-noise ratio (SNR, in db) is defined as $10\log_{10}\left(\frac{\|X\beta\|_2^2}{n\sigma_e^2}\right)$.

In our setting, $n = 120$, $p = 100$, $d = 5$.  $\beta$ is a length $100$ vector, in which $90$ are zero, and $10$ are $\left\{-5, -4, -3, -2, -1, 1, 2, 3, 4, 5\right\}$. $\text{SNR} = 20$.

```{r, cache = TRUE, echo = FALSE}
p = 100
n = 120
d = 5
SNR = 20
spa = 0.9
k = round(p * (1 - spa))
eta = 1 - k / n
sd_B = 1

set.seed(777)
B = matrix(rnorm(p * d, sd = sd_B), ncol = d, nrow = p)
V = B %*% t(B) + diag(p)
X = rmvnorm(n = n, sigma = V)
X = apply(X, 2, vec.norm)
beta = rep(0, p)
beta[sample(p, k)] = c(1:(k / 2), -(1:(k / 2)))
sd_noise = sqrt(mean((X %*% beta)^2) * 10^(- SNR / 10))
e = rnorm(n, sd = sd_noise)
y = X %*% beta
z = y + e

lambda = seq(0, 0.1, length = 20)
cv_mse = cv.sbr(A = X, z = z, lambda = lambda)
lambda.cv = lambda[which.min(cv_mse$mse_mean)]
lambda = exp(seq(log(lambda.cv / 2), log(lambda.cv * 4), length = 100))
L = length(lambda)
betahat = matrix(nrow = L, ncol = p)
for (i in 1:L) {
  sb = sbr(A = X, z = z, lambda = lambda[i])
  betahat[i, ] = sb$x
}


xlim = range(log(lambda))
ylim = c(min(min(betahat), 0), max(max(betahat), 0))
col = rainbow(p)
plot(lambda, rep(0, L), type = "n", xlim = xlim, ylim = ylim, xlab = expression(log(lambda)), ylab = "", main = expression(paste("Solution Path of ", L[0], " regularization by SBR")))
title(ylab = expression(hat(beta)), line = 2.5)
lambda = lambda[order(lambda)]
for (i in 1:p) {
  lines(log(lambda), betahat[order(lambda), i], col = col[i])
  abline(h = beta[i], col = col[i], lty = 3)
}
abline(v = log(lambda.cv), lty = 2)


fit.cv = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 1)
fit = glmnet::glmnet(x = X, y = z, intercept = FALSE, alpha = 1)
plot(fit, xvar = "lambda", ylim = c(-5, 5), xlab = expression(log(lambda)), main = "Solution Path of Lasso by glmnet", ylab = "", xlim = log(c(fit.cv$lambda.min / 2, fit.cv$lambda.min * 4)))
title(ylab = expression(hat(beta)), line = 2.5)
for (i in 1:p) {
  abline(h = beta[i], col = col[i], lty = 3)
}
abline(v = log(fit.cv$lambda.min), lty = 2)
```

## Performance comparison: SNR = 20

```{r, cache = TRUE, echo = FALSE}
p = 100
n = 120
d = 5
SNR = 20
spa = 0.9
k = round(p * (1 - spa))
eta = 1 - k / n
sd_B = 1
set.seed(777)
snr = MSE_ls = MSE_la = MSE_el = MSE_sb = MSE_bb = MSE_ss = tpn_el = tpn_la = tpn_sb = tpn_ss = fpn_el = fpn_la = fpn_sb = fpn_ss = c()


for (ii in 1:1000) {
B = matrix(rnorm(p * d, sd = sd_B), ncol = d, nrow = p)
V = B %*% t(B) + diag(p)
X = rmvnorm(n = n, sigma = V)
X = apply(X, 2, vec.norm)
beta = rep(0, p)
beta[sample(p, k)] = c(1:(k / 2), -(1:(k / 2)))
tp_beta = sparrep(beta)[, 1]

sd_noise = sqrt(mean((X %*% beta)^2) * 10^(- SNR / 10))
e = rnorm(n, sd = sd_noise)
y = X %*% beta
z = y + e

snr[ii] = 10 * log10(mean(y^2) / mean(e^2))


ls = lm(z ~ X - 1) # least squares
la = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 1) # lasso
el = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 0.5) # elastic net
lambda.sb = seq(0, 0.1, 0.01)
cv_mse = cv.sbr(A = X, z = z, lambda = lambda.sb)
lambda.sb.opt = lambda.sb[which.min(cv_mse$mse_mean)]
sb = sbr(A = X, z = z, lambda = lambda.sb.opt) # l0 with `SBR` using lambda from CV
ss.prior = BoomSpikeSlab::SpikeSlabPrior(X, z, expected.model.size = round(p * 0.5))
ss = BoomSpikeSlab::lm.spike(z ~ X - 1, niter = 1000, prior = ss.prior, ping = 0) #spike-and-slab with `BoomSpikeSlab`
capture.output(bb <- BayesBridge::bridge.reg(y = z, X, alpha = -1, nsamp = 1000), file = "/dev/null") # Bayesian bridge

MSE_ls[ii] = mean((coef(ls)- beta)^2)
MSE_la[ii] = mean((coef(la, s = "lambda.min")[-1] - beta)^2)
MSE_el[ii] = mean((coef(el, s = "lambda.min")[-1] - beta)^2)
MSE_bb[ii] = mean((colMeans(bb$beta) - beta)^2)
MSE_sb[ii] = mean((sb$x - beta)^2)
MSE_ss[ii] = mean((summary(ss, burn = 500, order = FALSE)$coef[, 1] - beta)^2)

varsel_la = sparrep(coef(la, s = "lambda.min")[-1])[, 1]
varsel_el = sparrep(coef(el, s = "lambda.min")[-1])[, 1]
varsel_sb = sparrep(sb$x)[, 1]
varsel_ss = (1:p)[summary(ss, burn = 500, order = FALSE)$coef[, 5] > 0.5]

tpn_la[ii] = varseleval(varsel_la)$tpn
fpn_la[ii] = varseleval(varsel_la)$fpn
tpn_el[ii] = varseleval(varsel_el)$tpn
fpn_el[ii] = varseleval(varsel_el)$fpn
tpn_ss[ii] = varseleval(varsel_ss)$tpn
fpn_ss[ii] = varseleval(varsel_ss)$fpn
tpn_sb[ii] = varseleval(varsel_sb)$tpn
fpn_sb[ii] = varseleval(varsel_sb)$fpn
}

res = cbind(snr, MSE_ls, MSE_la, MSE_el, MSE_bb, MSE_ss, MSE_sb, tpn_la, tpn_el, tpn_ss, tpn_sb, fpn_la, fpn_el, fpn_ss, fpn_sb)
write.table(res, "../output/res_0.9_20_5", quote = FALSE, row.names = FALSE)
```

```{r, cache = TRUE, echo = FALSE}
res = read.table("../output/res_0.9_20_5", header = TRUE)
par(cex.axis = 0.8)
par(mar = c(6.1, 2.5, 2.1, 1.1))
method.names = c("OLS", "LASSO", "Elastic Net", "Bayesian Bridge", "Spike & Slab", "SBR")

c1 <- rainbow(6)
c2 <- rainbow(6, alpha=0.2)
c3 <- rainbow(6, v=0.7)

boxplot(res[, 2:7], 
       names = method.names, las = 2, ylab = "Empirical MSE", main = "Comparison on Empirical Mean Squared Error", ylim = c(0, 0.4),
       col = c2, medcol = c3, whiskcol = c1, staplecol = c3, boxcol = c3, outcol = c3
       )

## boxplot(res[, 2:7], names = method.names, las = 2, ylab = "Empirical MSE", main = "Comparison on Empirical Mean Squared Error", col = 1:6, ylim = c(0, 0.4))

par(mfrow = c(1, 2))
c1 <- c1[c(2, 3, 5, 6)]
c2 <- c2[c(2, 3, 5, 6)]
c3 <- c3[c(2, 3, 5, 6)]

boxplot(res[, 8:11], 
names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of True Selection", las = 2, main = "Comparison on True Selection",
col = c2, medcol = c3, whiskcol = c1, staplecol = c3, boxcol = c3, outcol = c3)

# boxplot(res[, 8:11], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of True Positive", las = 2, main = "Comparison on True Positive")
abline(h = 10, lty = 2)
boxplot(res[, 12:15], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of False Selection", las = 2, main = "Comparison on False Selection",
       col = c2, medcol = c3, whiskcol = c1, staplecol = c3, boxcol = c3, outcol = c3
       )
# res = read.table("../output/res_0.9_20_5", header = TRUE)
# par(cex.axis = 0.8)
# par(mar = c(6.1, 2.5, 2.1, 1.1))
# method.names = c("OLS", "LASSO", "Elastic Net", "Bayesian Bridge", "Spike & Slab", "SBR")
# boxplot(res[, 2:7], names = method.names, las = 2, ylab = "Empirical MSE", main = "Empirical Mean Squared Error", ylim = c(0, 0.4))
# 
# par(mfrow = c(1, 2))
# boxplot(res[, 8:11], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of True Positive", las = 2, main = "Number of True Selection")
# abline(h = 10, lty = 2, col = "red")
# boxplot(res[, 12:15], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of False Positive", las = 2, main = "Number of False Selection")
```

## Performance comparison: SNR = 15

```{r, cache = TRUE, echo = FALSE}
p = 100
n = 120
d = 5
SNR = 15
spa = 0.9
k = round(p * (1 - spa))
eta = 1 - k / n
sd_B = 1
set.seed(777)
snr = MSE_ls = MSE_la = MSE_el = MSE_sb = MSE_bb = MSE_ss = tpn_el = tpn_la = tpn_sb = tpn_ss = fpn_el = fpn_la = fpn_sb = fpn_ss = c()


for (ii in 1:1000) {
B = matrix(rnorm(p * d, sd = sd_B), ncol = d, nrow = p)
V = B %*% t(B) + diag(p)
X = rmvnorm(n = n, sigma = V)
X = apply(X, 2, vec.norm)
beta = rep(0, p)
beta[sample(p, k)] = c(1:(k / 2), -(1:(k / 2)))
tp_beta = sparrep(beta)[, 1]

sd_noise = sqrt(mean((X %*% beta)^2) * 10^(- SNR / 10))
e = rnorm(n, sd = sd_noise)
y = X %*% beta
z = y + e

snr[ii] = 10 * log10(mean(y^2) / mean(e^2))


ls = lm(z ~ X - 1) # least squares
la = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 1) # lasso
el = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 0.5) # elastic net
lambda.sb = seq(0, 0.1, 0.01)
cv_mse = cv.sbr(A = X, z = z, lambda = lambda.sb)
lambda.sb.opt = lambda.sb[which.min(cv_mse$mse_mean)]
sb = sbr(A = X, z = z, lambda = lambda.sb.opt) # l0 with `SBR` using lambda from CV
ss.prior = BoomSpikeSlab::SpikeSlabPrior(X, z, expected.model.size = round(p * 0.5))
ss = BoomSpikeSlab::lm.spike(z ~ X - 1, niter = 1000, prior = ss.prior, ping = 0) #spike-and-slab with `BoomSpikeSlab`
capture.output(bb <- BayesBridge::bridge.reg(y = z, X, nsamp = 1000), file = "/dev/null") # Bayesian bridge

MSE_ls[ii] = mean((coef(ls)- beta)^2)
MSE_la[ii] = mean((coef(la, s = "lambda.min")[-1] - beta)^2)
MSE_el[ii] = mean((coef(el, s = "lambda.min")[-1] - beta)^2)
MSE_bb[ii] = mean((colMeans(bb$beta) - beta)^2)
MSE_sb[ii] = mean((sb$x - beta)^2)
MSE_ss[ii] = mean((summary(ss, burn = 500, order = FALSE)$coef[, 1] - beta)^2)

varsel_la = sparrep(coef(la, s = "lambda.min")[-1])[, 1]
varsel_el = sparrep(coef(el, s = "lambda.min")[-1])[, 1]
varsel_sb = sparrep(sb$x)[, 1]
varsel_ss = (1:p)[summary(ss, burn = 500, order = FALSE)$coef[, 5] > 0.5]

tpn_la[ii] = varseleval(varsel_la)$tpn
fpn_la[ii] = varseleval(varsel_la)$fpn
tpn_el[ii] = varseleval(varsel_el)$tpn
fpn_el[ii] = varseleval(varsel_el)$fpn
tpn_ss[ii] = varseleval(varsel_ss)$tpn
fpn_ss[ii] = varseleval(varsel_ss)$fpn
tpn_sb[ii] = varseleval(varsel_sb)$tpn
fpn_sb[ii] = varseleval(varsel_sb)$fpn
}

res = cbind(snr, MSE_ls, MSE_la, MSE_el, MSE_bb, MSE_ss, MSE_sb, tpn_la, tpn_el, tpn_ss, tpn_sb, fpn_la, fpn_el, fpn_ss, fpn_sb)
write.table(res, "../output/res_0.9_15_5", quote = FALSE, row.names = FALSE)
```

```{r, cache = TRUE, echo = FALSE}
res = read.table("../output/res_0.9_15_5", header = TRUE)
par(cex.axis = 0.8)
par(mar = c(6.1, 2.5, 2.1, 1.1))
method.names = c("OLS", "LASSO", "Elastic Net", "Bayesian Bridge", "Spike & Slab", "SBR")
boxplot(res[, 2:7], names = method.names, las = 2, ylab = "Empirical MSE", main = "Comparison on Empirical Mean Squared Error")

par(mfrow = c(1, 2))
boxplot(res[, 8:11], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of True Positive", las = 2, main = "Comparison on True Positive")
abline(h = 10, lty = 2, col = "red")
boxplot(res[, 12:15], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of False Positive", las = 2, main = "Comparison on False Positive")
```

## Performance comparison: SNR = 10

```{r, cache = TRUE, echo = FALSE}
p = 100
n = 120
d = 5
SNR = 10
spa = 0.9
k = round(p * (1 - spa))
eta = 1 - k / n
sd_B = 1
set.seed(777)
snr = MSE_ls = MSE_la = MSE_el = MSE_sb = MSE_bb = MSE_ss = tpn_el = tpn_la = tpn_sb = tpn_ss = fpn_el = fpn_la = fpn_sb = fpn_ss = c()


for (ii in 1:1000) {
B = matrix(rnorm(p * d, sd = sd_B), ncol = d, nrow = p)
V = B %*% t(B) + diag(p)
X = rmvnorm(n = n, sigma = V)
X = apply(X, 2, vec.norm)
beta = rep(0, p)
beta[sample(p, k)] = c(1:(k / 2), -(1:(k / 2)))
tp_beta = sparrep(beta)[, 1]

sd_noise = sqrt(mean((X %*% beta)^2) * 10^(- SNR / 10))
e = rnorm(n, sd = sd_noise)
y = X %*% beta
z = y + e

snr[ii] = 10 * log10(mean(y^2) / mean(e^2))


ls = lm(z ~ X - 1) # least squares
la = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 1) # lasso
el = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 0.5) # elastic net
lambda.sb = seq(0, 0.1, 0.01)
cv_mse = cv.sbr(A = X, z = z, lambda = lambda.sb)
lambda.sb.opt = lambda.sb[which.min(cv_mse$mse_mean)]
sb = sbr(A = X, z = z, lambda = lambda.sb.opt) # l0 with `SBR` using lambda from CV
ss.prior = BoomSpikeSlab::SpikeSlabPrior(X, z, expected.model.size = round(p * 0.5))
ss = BoomSpikeSlab::lm.spike(z ~ X - 1, niter = 1000, prior = ss.prior, ping = 0) #spike-and-slab with `BoomSpikeSlab`
capture.output(bb <- BayesBridge::bridge.reg(y = z, X, nsamp = 1000), file = "/dev/null") # Bayesian bridge

MSE_ls[ii] = mean((coef(ls)- beta)^2)
MSE_la[ii] = mean((coef(la, s = "lambda.min")[-1] - beta)^2)
MSE_el[ii] = mean((coef(el, s = "lambda.min")[-1] - beta)^2)
MSE_bb[ii] = mean((colMeans(bb$beta) - beta)^2)
MSE_sb[ii] = mean((sb$x - beta)^2)
MSE_ss[ii] = mean((summary(ss, burn = 500, order = FALSE)$coef[, 1] - beta)^2)

varsel_la = sparrep(coef(la, s = "lambda.min")[-1])[, 1]
varsel_el = sparrep(coef(el, s = "lambda.min")[-1])[, 1]
varsel_sb = sparrep(sb$x)[, 1]
varsel_ss = (1:p)[summary(ss, burn = 500, order = FALSE)$coef[, 5] > 0.5]

tpn_la[ii] = varseleval(varsel_la)$tpn
fpn_la[ii] = varseleval(varsel_la)$fpn
tpn_el[ii] = varseleval(varsel_el)$tpn
fpn_el[ii] = varseleval(varsel_el)$fpn
tpn_ss[ii] = varseleval(varsel_ss)$tpn
fpn_ss[ii] = varseleval(varsel_ss)$fpn
tpn_sb[ii] = varseleval(varsel_sb)$tpn
fpn_sb[ii] = varseleval(varsel_sb)$fpn
}

res = cbind(snr, MSE_ls, MSE_la, MSE_el, MSE_bb, MSE_ss, MSE_sb, tpn_la, tpn_el, tpn_ss, tpn_sb, fpn_la, fpn_el, fpn_ss, fpn_sb)
write.table(res, "../output/res_0.9_10_5", quote = FALSE, row.names = FALSE)
```

```{r, cache = TRUE, echo = FALSE}
res = read.table("../output/res_0.9_10_5", header = TRUE)
par(cex.axis = 0.8)
par(mar = c(6.1, 2.5, 2.1, 1.1))
method.names = c("OLS", "LASSO", "Elastic Net", "Bayesian Bridge", "Spike & Slab", "SBR")
boxplot(res[, 2:7], names = method.names, las = 2, ylab = "Empirical MSE", main = "Comparison on Empirical Mean Squared Error")

par(mfrow = c(1, 2))
boxplot(res[, 8:11], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of True Positive", las = 2, main = "Comparison on True Positive")
abline(h = 10, lty = 2, col = "red")
boxplot(res[, 12:15], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of False Positive", las = 2, main = "Comparison on False Positive")
```

## Performance comparison: SNR = 5

```{r, cache = TRUE, echo = FALSE}
p = 100
n = 120
d = 5
SNR = 5
spa = 0.9
k = round(p * (1 - spa))
eta = 1 - k / n
sd_B = 1
set.seed(777)
snr = MSE_ls = MSE_la = MSE_el = MSE_sb = MSE_bb = MSE_ss = tpn_el = tpn_la = tpn_sb = tpn_ss = fpn_el = fpn_la = fpn_sb = fpn_ss = c()


for (ii in 1:1000) {
B = matrix(rnorm(p * d, sd = sd_B), ncol = d, nrow = p)
V = B %*% t(B) + diag(p)
X = rmvnorm(n = n, sigma = V)
X = apply(X, 2, vec.norm)
beta = rep(0, p)
beta[sample(p, k)] = c(1:(k / 2), -(1:(k / 2)))
tp_beta = sparrep(beta)[, 1]

sd_noise = sqrt(mean((X %*% beta)^2) * 10^(- SNR / 10))
e = rnorm(n, sd = sd_noise)
y = X %*% beta
z = y + e

snr[ii] = 10 * log10(mean(y^2) / mean(e^2))


ls = lm(z ~ X - 1) # least squares
la = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 1) # lasso
el = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 0.5) # elastic net
lambda.sb = seq(0, 0.1, 0.01)
cv_mse = cv.sbr(A = X, z = z, lambda = lambda.sb)
lambda.sb.opt = lambda.sb[which.min(cv_mse$mse_mean)]
sb = sbr(A = X, z = z, lambda = lambda.sb.opt) # l0 with `SBR` using lambda from CV
ss.prior = BoomSpikeSlab::SpikeSlabPrior(X, z, expected.model.size = round(p * 0.5))
ss = BoomSpikeSlab::lm.spike(z ~ X - 1, niter = 1000, prior = ss.prior, ping = 0) #spike-and-slab with `BoomSpikeSlab`
capture.output(bb <- BayesBridge::bridge.reg(y = z, X, nsamp = 1000), file = "/dev/null") # Bayesian bridge

MSE_ls[ii] = mean((coef(ls)- beta)^2)
MSE_la[ii] = mean((coef(la, s = "lambda.min")[-1] - beta)^2)
MSE_el[ii] = mean((coef(el, s = "lambda.min")[-1] - beta)^2)
MSE_bb[ii] = mean((colMeans(bb$beta) - beta)^2)
MSE_sb[ii] = mean((sb$x - beta)^2)
MSE_ss[ii] = mean((summary(ss, burn = 500, order = FALSE)$coef[, 1] - beta)^2)

varsel_la = sparrep(coef(la, s = "lambda.min")[-1])[, 1]
varsel_el = sparrep(coef(el, s = "lambda.min")[-1])[, 1]
varsel_sb = sparrep(sb$x)[, 1]
varsel_ss = (1:p)[summary(ss, burn = 500, order = FALSE)$coef[, 5] > 0.5]

tpn_la[ii] = varseleval(varsel_la)$tpn
fpn_la[ii] = varseleval(varsel_la)$fpn
tpn_el[ii] = varseleval(varsel_el)$tpn
fpn_el[ii] = varseleval(varsel_el)$fpn
tpn_ss[ii] = varseleval(varsel_ss)$tpn
fpn_ss[ii] = varseleval(varsel_ss)$fpn
tpn_sb[ii] = varseleval(varsel_sb)$tpn
fpn_sb[ii] = varseleval(varsel_sb)$fpn
}

res = cbind(snr, MSE_ls, MSE_la, MSE_el, MSE_bb, MSE_ss, MSE_sb, tpn_la, tpn_el, tpn_ss, tpn_sb, fpn_la, fpn_el, fpn_ss, fpn_sb)
write.table(res, "../output/res_0.9_5_5", quote = FALSE, row.names = FALSE)
```

```{r, cache = TRUE, echo = FALSE}
res = read.table("../output/res_0.9_5_5", header = TRUE)
par(cex.axis = 0.8)
par(mar = c(6.1, 2.5, 2.1, 1.1))
method.names = c("OLS", "LASSO", "Elastic Net", "Bayesian Bridge", "Spike & Slab", "SBR")
boxplot(res[, 2:7], names = method.names, las = 2, ylab = "Empirical MSE", main = "Comparison on Empirical Mean Squared Error")

par(mfrow = c(1, 2))
boxplot(res[, 8:11], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of True Positive", las = 2, main = "Comparison on True Positive")
abline(h = 10, lty = 2, col = "red")
boxplot(res[, 12:15], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of False Positive", las = 2, main = "Comparison on False Positive")
```

## Performance comparison: SNR = 0

```{r, cache = TRUE, echo = FALSE}
p = 100
n = 120
d = 5
SNR = 0
spa = 0.9
k = round(p * (1 - spa))
eta = 1 - k / n
sd_B = 1
set.seed(777)
snr = MSE_ls = MSE_la = MSE_el = MSE_sb = MSE_bb = MSE_ss = tpn_el = tpn_la = tpn_sb = tpn_ss = fpn_el = fpn_la = fpn_sb = fpn_ss = c()


for (ii in 1:1000) {
B = matrix(rnorm(p * d, sd = sd_B), ncol = d, nrow = p)
V = B %*% t(B) + diag(p)
X = rmvnorm(n = n, sigma = V)
X = apply(X, 2, vec.norm)
beta = rep(0, p)
beta[sample(p, k)] = c(1:(k / 2), -(1:(k / 2)))
tp_beta = sparrep(beta)[, 1]

sd_noise = sqrt(mean((X %*% beta)^2) * 10^(- SNR / 10))
e = rnorm(n, sd = sd_noise)
y = X %*% beta
z = y + e

snr[ii] = 10 * log10(mean(y^2) / mean(e^2))


ls = lm(z ~ X - 1) # least squares
la = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 1) # lasso
el = glmnet::cv.glmnet(x = X, y = z, intercept = FALSE, alpha = 0.5) # elastic net
lambda.sb = seq(0, 0.1, 0.01)
cv_mse = cv.sbr(A = X, z = z, lambda = lambda.sb)
lambda.sb.opt = lambda.sb[which.min(cv_mse$mse_mean)]
sb = sbr(A = X, z = z, lambda = lambda.sb.opt) # l0 with `SBR` using lambda from CV
ss.prior = BoomSpikeSlab::SpikeSlabPrior(X, z, expected.model.size = round(p * 0.5))
ss = BoomSpikeSlab::lm.spike(z ~ X - 1, niter = 1000, prior = ss.prior, ping = 0) #spike-and-slab with `BoomSpikeSlab`
capture.output(bb <- BayesBridge::bridge.reg(y = z, X, nsamp = 1000), file = "/dev/null") # Bayesian bridge

MSE_ls[ii] = mean((coef(ls)- beta)^2)
MSE_la[ii] = mean((coef(la, s = "lambda.min")[-1] - beta)^2)
MSE_el[ii] = mean((coef(el, s = "lambda.min")[-1] - beta)^2)
MSE_bb[ii] = mean((colMeans(bb$beta) - beta)^2)
MSE_sb[ii] = mean((sb$x - beta)^2)
MSE_ss[ii] = mean((summary(ss, burn = 500, order = FALSE)$coef[, 1] - beta)^2)

varsel_la = sparrep(coef(la, s = "lambda.min")[-1])[, 1]
varsel_el = sparrep(coef(el, s = "lambda.min")[-1])[, 1]
varsel_sb = sparrep(sb$x)[, 1]
varsel_ss = (1:p)[summary(ss, burn = 500, order = FALSE)$coef[, 5] > 0.5]

tpn_la[ii] = varseleval(varsel_la)$tpn
fpn_la[ii] = varseleval(varsel_la)$fpn
tpn_el[ii] = varseleval(varsel_el)$tpn
fpn_el[ii] = varseleval(varsel_el)$fpn
tpn_ss[ii] = varseleval(varsel_ss)$tpn
fpn_ss[ii] = varseleval(varsel_ss)$fpn
tpn_sb[ii] = varseleval(varsel_sb)$tpn
fpn_sb[ii] = varseleval(varsel_sb)$fpn
}

res = cbind(snr, MSE_ls, MSE_la, MSE_el, MSE_bb, MSE_ss, MSE_sb, tpn_la, tpn_el, tpn_ss, tpn_sb, fpn_la, fpn_el, fpn_ss, fpn_sb)
write.table(res, "../output/res_0.9_0_5", quote = FALSE, row.names = FALSE)
```

```{r, cache = TRUE, echo = FALSE}
res = read.table("../output/res_0.9_0_5", header = TRUE)
par(cex.axis = 0.8)
par(mar = c(6.1, 2.5, 2.1, 1.1))
method.names = c("OLS", "LASSO", "Elastic Net", "Bayesian Bridge", "Spike & Slab", "SBR")
boxplot(res[, 2:7], names = method.names, las = 2, ylab = "Empirical MSE", main = "Comparison on Empirical Mean Squared Error")

par(mfrow = c(1, 2))
boxplot(res[, 8:11], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of True Positive", las = 2, main = "Comparison on True Positive")
abline(h = 10, lty = 2, col = "red")
boxplot(res[, 12:15], names = c("LASSO", "Elastic Net", "Spike & Slab", "SBR"), ylab = "Number of False Positive", las = 2, main = "Comparison on False Positive")
```

[`BayesBridge`]: https://cran.r-project.org/web/packages/BayesBridge/index.html
[Polson et al., 2014]: http://onlinelibrary.wiley.com/doi/10.1111/rssb.12042/suppinfo
[`BoomSpikeSlab`]: https://cran.r-project.org/web/packages/BoomSpikeSlab/index.html

## Session Information

```{r session-info}
```
